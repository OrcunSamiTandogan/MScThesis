{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Step 10: Predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- `Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    if manager == 1:\n",
    "        print(\"s1_load.ipynb running from MANAGER\")\n",
    "except: \n",
    "    %run s0_config.ipynb \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-) `Load`\n",
    "- 0-) Default\n",
    "- 1-) SVM\n",
    "- 2-) SGD\n",
    "- 3-) Decision Tree\n",
    "- 4-) Random Forest\n",
    "- 5- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-) `Default`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic \n",
    "import_name = \"subject_outlier\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_prediction}\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    subject_outlier = pickle.load(file)   \n",
    "\n",
    "import_name = \"subject_autoencoder\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_prediction}\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    subject_autoencoder = pickle.load(file)   \n",
    "\n",
    "import_name = \"subject_dimension\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_prediction}\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    subject_dimension = pickle.load(file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-) `NN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "import_name = \"NN_All_Models\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    NN_All_Models = pickle.load(file)   \n",
    "\n",
    "# Subject\n",
    "import_name = \"NN_All_Encoders\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    NN_All_Encoders = pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-) `SVM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "import_name = \"SVM_All_Models\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    SVM_All_Models = pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-) `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "import_name = \"SGD_All_Models\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    SGD_All_Models = pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-) `Decision Tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "import_name = \"DT_All_Models\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    DT_All_Models = pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-) `Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject\n",
    "import_name = \"RF_All_Models\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/model\", f\"{import_name}\" + \".pkl\"))  , 'rb') as file: \n",
    "    RF_All_Models = pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.A- `Prediction`\n",
    "- 0-) Config\n",
    "- 1-) Text to Predict\n",
    "- 2-) Preprocess Text\n",
    "- 3-) Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0- `Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text format for prediction\n",
    "gene_marker = \"JCHAIN\"  # 'Unknown' acts as a placeholder for Cell_Type \n",
    "gene_expression = 6.496516  # Example expression level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- `Key to Predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model     = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "text_to_predict = \"Unknown \" + gene_marker\n",
    "inputs = tokenizer(text_to_predict, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate text embedding\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    text_embedding = outputs.last_hidden_state.mean(dim=1).numpy()  # Convert to numpy array for compatibility\n",
    "\n",
    "\n",
    "\n",
    "gene_expression_array = np.array([[gene_expression]])  # Shape (1, 1) to match embedding shape for concatenation\n",
    "\n",
    "# Combine gene expression and text embeddings\n",
    "combined_input = np.hstack([gene_expression_array, text_embedding])\n",
    "# Split combined_input back into gene_expression_array and text_embedding for prediction\n",
    "gene_expression_array = combined_input[:, :1]  # Assuming gene expression is the first column\n",
    "text_embedding = combined_input[:, 1:]  # The rest are the text embedding columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_NN       = 0  \n",
    "NN_All_Model   = NN_All_Models[index_NN] \n",
    "NN_All_Encoder = NN_All_Encoders[index_NN]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_All_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions using the split inputs\n",
    "prediction = NN_All_Model.predict([gene_expression_array, text_embedding])\n",
    "\n",
    "# Proceed with the rest of your prediction process\n",
    "predicted_index = np.argmax(prediction, axis=1)\n",
    "predicted_label = NN_All_Encoder.inverse_transform(predicted_index)\n",
    "print(\"Predicted Cell Type Label:\", predicted_label[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4-B `Prediction`\n",
    "- 0-) Config\n",
    "- 1-) Text to Predict\n",
    "- 2-) Preprocess Text\n",
    "- 3-) Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0- `Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- `Key to Predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "# Assuming 'gene_marker' is the gene marker and 'gene_expression' is the expression level for prediction\n",
    "gene_marker = \"JCHAIN\"\n",
    "gene_expression = 6.496516\t # Example expression level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- `Preprocess Text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text embeddings\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "inputs = tokenizer(\"Unknown \" + gene_marker, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Combine gene expression and text embeddings\n",
    "combined_input = np.hstack([np.array([[gene_expression]]), embedding])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- `Prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the SVM model\n",
    "index_SVM = 0  # Assuming you have a list of SVM models and you want to use the first one\n",
    "SVM_All_Model = SVM_All_Models[index_SVM]  # Assuming SVM_All_Models is a list of trained SVM models\n",
    "prediction = SVM_All_Model.predict(combined_input)\n",
    "\n",
    "print(\"Predicted Cell Type:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_SVM = 0  \n",
    "SGD_All_Model = SGD_All_Models[index_SVM] \n",
    "prediction = SGD_All_Model.predict(combined_input)\n",
    "\n",
    "print(\"Predicted Cell Type:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_SVM = 0  \n",
    "DT_All_Model = DT_All_Models[index_SVM] \n",
    "prediction   = DT_All_Models.predict(combined_input)\n",
    "\n",
    "print(\"Predicted Cell Type:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_SVM = 0  \n",
    "RF_All_Model = RF_All_Models[index_SVM] \n",
    "prediction   = RF_All_Model.predict(combined_input)\n",
    "\n",
    "print(\"Predicted Cell Type:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    index_SVM = 0  \n",
    "    GB_All_Model = GB_All_Models[index_SVM] \n",
    "    prediction = GB_All_Model.predict(combined_input)\n",
    "\n",
    "    print(\"Predicted Cell Type:\", prediction[0])\n",
    "except:\n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilimlan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
