{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tools of Statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section is bypassed as autoencoder models are worked so fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_plot_pca(PCA_DF):\n",
    "    # Initialize an empty list to hold the best models\n",
    "    PCA_best_models = []\n",
    "\n",
    "    # Filter for 'nc2' models and find the best one based on accuracy\n",
    "    PCA_nc2_df = PCA_DF[PCA_DF['Model'].str.contains('n2')]\n",
    "    if not PCA_nc2_df.empty:\n",
    "        best_nc2 = PCA_nc2_df.loc[PCA_nc2_df['Accuracy'].idxmax()]\n",
    "        PCA_best_models.append(best_nc2)\n",
    "\n",
    "    # Filter for 'nc3' models and find the best one based on accuracy\n",
    "    PCA_nc3_df = PCA_DF[PCA_DF['Model'].str.contains('n3')]\n",
    "    if not PCA_nc3_df.empty:\n",
    "        best_nc3 = PCA_nc3_df.loc[PCA_nc3_df['Accuracy'].idxmax()]\n",
    "        PCA_best_models.append(best_nc3)\n",
    "\n",
    "    if PCA_best_models:  # Check if the list is not empty\n",
    "        PCA_best_models_DF = pd.DataFrame(PCA_best_models).reset_index(drop=True) \n",
    "    # Result  \n",
    "    return PCA_best_models_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_plot_tsne(TSNE_DF):\n",
    "    # Initialize an empty list to hold the best models\n",
    "    TSNE_best_models = []\n",
    "\n",
    "    # Filter for 'nc2' models and find the best one based on accuracy\n",
    "    TSNE_nc2_df = TSNE_DF[TSNE_DF['Model'].str.contains('nc2')]\n",
    "    if not TSNE_nc2_df.empty:\n",
    "        best_nc2 = TSNE_nc2_df.loc[TSNE_nc2_df['Accuracy'].idxmax()]\n",
    "        TSNE_best_models.append(best_nc2)\n",
    "\n",
    "    # Filter for 'nc3' models and find the best one based on accuracy\n",
    "    TSNE_nc3_df = TSNE_DF[TSNE_DF['Model'].str.contains('nc3')]\n",
    "    if not TSNE_nc3_df.empty:\n",
    "        best_nc3 = TSNE_nc3_df.loc[TSNE_nc3_df['Accuracy'].idxmax()]\n",
    "        TSNE_best_models.append(best_nc3)\n",
    "\n",
    "    if TSNE_best_models:  # Check if the list is not empty\n",
    "        TSNE_best_models_DF = pd.DataFrame(TSNE_best_models).reset_index(drop=True) \n",
    "    # Result  \n",
    "    return TSNE_best_models_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_plot_umap(UMAP_DF):\n",
    "    # Initialize an empty list to hold the best models\n",
    "    UMAP_best_models = []\n",
    "\n",
    "    # Filter for 'nc2' models and find the best one based on accuracy\n",
    "    UMAP_nc2_df = UMAP_DF[UMAP_DF['Model'].str.contains('nc2')]\n",
    "    if not UMAP_nc2_df.empty:\n",
    "        best_nc2 = UMAP_nc2_df.loc[UMAP_nc2_df['Accuracy'].idxmax()]\n",
    "        UMAP_best_models.append(best_nc2)\n",
    "\n",
    "    # Filter for 'nc3' models and find the best one based on accuracy\n",
    "    UMAP_nc3_df = UMAP_DF[UMAP_DF['Model'].str.contains('nc3')]\n",
    "    if not UMAP_nc3_df.empty:\n",
    "        best_nc3 = UMAP_nc3_df.loc[UMAP_nc3_df['Accuracy'].idxmax()]\n",
    "        UMAP_best_models.append(best_nc3)\n",
    "\n",
    "    if UMAP_best_models:  # Check if the list is not empty\n",
    "        UMAP_best_models_DF = pd.DataFrame(UMAP_best_models).reset_index(drop=True) \n",
    "    # Result  \n",
    "    return UMAP_best_models_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(true_labels, pred_labels): \n",
    "    try: \n",
    "        # (1/3)\n",
    "        true_labels_series = pd.Series(true_labels)\n",
    "        pred_labels_series = pd.Series(pred_labels)\n",
    "        # (2/3)\n",
    "        unique_true_labels = pd.unique(true_labels_series)\n",
    "        unique_pred_labels = pd.unique(pred_labels_series) \n",
    "        # (3/3) \n",
    "        cm    = confusion_matrix(true_labels, pred_labels)\n",
    "        cm_DF = pd.DataFrame(cm, columns = [ str(i) for i in unique_pred_labels], index = [ str(i) for i in unique_true_labels ]) \n",
    "        cm_DF = cm_DF.rename_axis(\"True\", axis=\"index\").rename_axis(\"Pred\", axis=\"columns\") \n",
    "    except:\n",
    "        cm    = confusion_matrix(true_labels, pred_labels) \n",
    "        cm_DF = pd.DataFrame(cm)\n",
    "        cm_DF = cm_DF.rename_axis(\"True\", axis=\"index\").rename_axis(\"Pred\", axis=\"columns\") \n",
    "    # Return \n",
    "    return cm_DF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(true_labels, pred_labels, model_name): \n",
    "    accuracy = accuracy_score(true_labels, pred_labels) \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "    basic_statistics = pd.DataFrame({ \"Model\": [f\"{model_name}\"],\n",
    "                                       \"Accuracy\": [accuracy],\n",
    "                                       \"Precision\": [precision],\n",
    "                                       \"Recall\": [recall],\n",
    "                                       \"F1\": [f1]})\n",
    "    return basic_statistics  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dimension_free_analysis(Dimension_Free_Dict_List, true_labels):\n",
    "    Dimension_Free_DF = pd.DataFrame()\n",
    "    for subject_name, subject_data in Dimension_Free_Dict_List.items():\n",
    "        pred_labels = subject_data[\"predictions\"]\n",
    "\n",
    "        # Calculate and store statistics\n",
    "        basic_statistics = calculate_statistics(true_labels=true_labels, pred_labels=pred_labels, model_name=subject_name)\n",
    "        Dimension_Free_DF = pd.concat([Dimension_Free_DF, basic_statistics], axis=0)\n",
    "        Dimension_Free_Dict_List[subject_name][\"statistics\"] = basic_statistics\n",
    "\n",
    "        # Calculate and store confusion matrix\n",
    "        cm_DF = calculate_confusion_matrix(true_labels=true_labels, pred_labels=pred_labels)\n",
    "        Dimension_Free_Dict_List[subject_name][\"confusion_matrix\"] = cm_DF\n",
    "\n",
    "        # Store model name\n",
    "        Dimension_Free_Dict_List[subject_name][\"model_name\"] = subject_name\n",
    "\n",
    "    Dimension_Free_DF.reset_index(drop=True, inplace=True)\n",
    "    return Dimension_Free_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
