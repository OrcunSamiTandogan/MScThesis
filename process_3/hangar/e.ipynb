{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'subject_data_full' is your DataFrame\n",
    "\n",
    "# Extract embeddings as lists of NumPy arrays\n",
    "cell_type_embeddings = subject_data_full[\"Cell_Type_Embeddings_Outlier\"].apply(np.array).tolist()\n",
    "gene_marker_embeddings = subject_data_full[\"Gene_Marker_Embeddings_Outlier\"].apply(np.array).tolist()\n",
    "\n",
    "# Verify that all embeddings have the expected shape, e.g., (768,) for cell type and (768,) for gene marker\n",
    "# This step is crucial and you might need to adjust depending on your actual data shape\n",
    "\n",
    "# Concatenate embeddings row-wise\n",
    "concat_embeddings = [np.concatenate([cell_emb, gene_emb]) for cell_emb, gene_emb in zip(cell_type_embeddings, gene_marker_embeddings)]\n",
    "\n",
    "# Correct approach, already in your code\n",
    "concat_embeddings_matrix = np.stack(concat_embeddings)\n",
    "concat_embeddings_tensor = torch.tensor(concat_embeddings_matrix).float()  # This should not trigger the warning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder combined dropout\n",
    "    @staticmethod\n",
    "    def combined_dropout(target_matrix, \n",
    "                        model_type         = 'combined_dropout',\n",
    "                        variable_lr        = [1e-3, 1e-4] ,\n",
    "                        batch_size         = 128,\n",
    "                        num_epochs         = 5,\n",
    "                        noise_factor       = [0.3,0.4],\n",
    "                        dropout_rate       = [0.1,0.2],\n",
    "                        input_outer_layer  = 1536,\n",
    "                        inner_layer        = 64 ,\n",
    "                        output_outer_layer = 100, \n",
    "                         ):\n",
    "        # variables: learning rate, noise_factor, dropout_rate \n",
    "\n",
    "        # Outputs B4_a, B4_b, B4_c, B4_d, B4_e, B4_f, B4_g, B4_h \n",
    "        print(\"Control Point | staticmethod combined_dropout | 4.0\") \n",
    "        X0_a_manager = AutoencoderManager(  model_type   = model_type,\n",
    "                                    lr           = variable_lr[0], \n",
    "                                    batch_size   = batch_size, \n",
    "                                    num_epochs   = num_epochs, \n",
    "                                    noise_factor = noise_factor[0],\n",
    "                                    dropout_rate = dropout_rate[0], \n",
    "                                    outer_layer         = input_outer_layer, \n",
    "                                    inner_layer         = inner_layer,\n",
    "                                    output_outer_layer  = output_outer_layer ,\n",
    "                                    ) \n",
    "        print(\"Control Point | staticmethod combined_dropout | 4.1\") \n",
    "        X0_a_manager.load_data( data_matrix = target_matrix ) \n",
    "        print(\"Control Point | staticmethod combined_dropout | 4.2\") \n",
    "        X0_a_manager.train_combined_dropout() \n",
    "        print(\"Control Point | staticmethod combined_dropout | 4.3\") \n",
    "        X0_a_reconstructed_data = X0_a_manager.get_reconstructed_data_combined_dropout(target_matrix) \n",
    "        print(\"Control Point | staticmethod combined_dropout | 4.4\") \n",
    "\n",
    "        B4_List = X0_a_reconstructed_data \n",
    "        return B4_List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Good one \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'Expression_Embeddings' column where first element is gene expression and the rest are embedding dimensions\n",
    "features = subject_data_full['Expression_Embeddings'].tolist()\n",
    "expressions = np.array([feature[0] for feature in features])  # Gene expression data\n",
    "embeddings = np.array([feature[1] for feature in features])  # Text embeddings\n",
    "\n",
    "# Preprocess labels\n",
    "cell_types = subject_data_full['Cell_Type'].values\n",
    "encoder = LabelEncoder()\n",
    "cell_types_encoded = encoder.fit_transform(cell_types)\n",
    "cell_types_categorical = to_categorical(cell_types_encoded)\n",
    "\n",
    "# Split data\n",
    "X_train_expr, X_test_expr, X_train_emb, X_test_emb, y_train, y_test = train_test_split(\n",
    "    expressions, embeddings, cell_types_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Input layers\n",
    "input_expr = Input(shape=(1,), name='input_expr')  # Gene expression input\n",
    "input_emb = Input(shape=(X_train_emb.shape[1],), name='input_emb')  # Text embeddings input\n",
    "\n",
    "# Separate pathways\n",
    "pathway_expr = Dense(32, activation='relu')(input_expr)\n",
    "pathway_emb = Dense(128, activation='relu')(input_emb)\n",
    "\n",
    "# Concatenate the pathways\n",
    "concatenated = Concatenate()([pathway_expr, pathway_emb])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(cell_types_categorical.shape[1], activation='softmax')(concatenated)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=[input_expr, input_emb], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [X_train_expr, X_train_emb], y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_expr, X_test_emb], y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_features(row):\n",
    "    expression, embeddings = row[0], row[1]\n",
    "    # Ensure the expression is the first item followed by all embedding values\n",
    "    return [expression] + embeddings\n",
    "\n",
    "#combined_features = np.hstack((expressions_normalization_process, embeddings_normalization_process))\n",
    "\n",
    "Subject_Out_1_Nor_0_Aut_0_DR_0 = np.array(subject_data_full['Expression_Embeddings'].apply(flatten_features).tolist())\n",
    "Subject_Out_1_Nor_1_Aut_0_DR_0 = np.array(subject_data_full['Expression_Embeddings_Normalized'].apply(flatten_features).tolist())\n",
    "Subject_Out_1_Nor_1_Aut_1_DR_0 = np.array(subject_data_full['Expression_Embeddings_Autoencoder'].apply(flatten_features).tolist())\n",
    "Subject_Out_1_Nor_1_Aut_1_DR_1 = np.array(subject_data_full['Expression_Embeddings_Dimension'].apply(flatten_features).tolist()) \n",
    "Subject_Out_1_Nor_1_Aut_1_DR_1\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming `subject_data_full` is your DataFrame and already contains \"Expression_Embeddings\" as combined features\n",
    "X = np.array(subject_data_full['Expression_Embeddings'].apply(lambda x: [x[0]] + x[1]).tolist())\n",
    "\n",
    "# Encode Cell_Type labels to integers and then to one-hot vectors\n",
    "le = LabelEncoder()\n",
    "y_integers = le.fit_transform(subject_data_full['Cell_Type'])\n",
    "y_onehot = to_categorical(y_integers)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(X.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y_onehot.shape[1], activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "Subject_Out_1_Nor_0_Aut_0_DR_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = subject_data_full['Expression_Embeddings'].tolist()\n",
    "expressions = np.array([feature[0] for feature in features])\n",
    "embeddings = np.array([feature[1] for feature in features])\n",
    "\n",
    "cell_types = subject_data_full['Cell_Type'].values\n",
    "encoder = LabelEncoder()\n",
    "cell_types_encoded = encoder.fit_transform(cell_types)\n",
    "cell_types_categorical = to_categorical(cell_types_encoded)\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(expressions), start=1):\n",
    "    X_train_expr, X_test_expr = expressions[train_index], expressions[test_index]\n",
    "    X_train_emb, X_test_emb = embeddings[train_index], embeddings[test_index]\n",
    "    y_train, y_test = cell_types_categorical[train_index], cell_types_categorical[test_index]\n",
    "\n",
    "    input_expr = Input(shape=(1,), name='input_expr')\n",
    "    input_emb = Input(shape=(X_train_emb.shape[1],), name='input_emb')\n",
    "    \n",
    "    noisy_expr = GaussianNoise(0.1)(input_expr)\n",
    "    noisy_emb = GaussianNoise(0.1)(input_emb)\n",
    "\n",
    "    pathway_expr = Dense(64, activation='relu')(noisy_expr)\n",
    "    pathway_expr = Dropout(0.3)(pathway_expr)\n",
    "    pathway_expr = Dense(32, activation='relu')(pathway_expr)\n",
    "\n",
    "    pathway_emb = Dense(256, activation='relu')(noisy_emb)\n",
    "    pathway_emb = Dropout(0.3)(pathway_emb)\n",
    "    pathway_emb = Dense(128, activation='relu')(pathway_emb)\n",
    "    \n",
    "    concatenated = Concatenate()([pathway_expr, pathway_emb])\n",
    "\n",
    "    output = Dense(cell_types_categorical.shape[1], activation='softmax')(concatenated)\n",
    "\n",
    "    model = Model(inputs=[input_expr, input_emb], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if fold == 1:\n",
    "        # Only print the model summary for the first fold\n",
    "        model.summary()\n",
    "\n",
    "    print(f\"\\nTraining on fold {fold}...\")\n",
    "    model.fit([X_train_expr, X_train_emb], y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)\n",
    "\n",
    "    loss, accuracy = model.evaluate([X_test_expr, X_test_emb], y_test, verbose=1)\n",
    "    print(f\"Fold {fold} Test Accuracy: {accuracy}\\n\")\n",
    "    scores.append(accuracy)\n",
    "\n",
    "average_score = np.mean(scores)\n",
    "print(f\"\\nAverage Test Accuracy across {n_splits} folds: {average_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = subject_data_full['Expression_Embeddings'].tolist()\n",
    "expressions = np.array([feature[0] for feature in features])\n",
    "embeddings = np.array([feature[1] for feature in features])\n",
    "\n",
    "cell_types = subject_data_full['Cell_Type'].values\n",
    "encoder = LabelEncoder()\n",
    "cell_types_encoded = encoder.fit_transform(cell_types)\n",
    "cell_types_categorical = to_categorical(cell_types_encoded)\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(expressions), start=1):\n",
    "    X_train_expr, X_test_expr = expressions[train_index], expressions[test_index]\n",
    "    X_train_emb, X_test_emb = embeddings[train_index], embeddings[test_index]\n",
    "    y_train, y_test = cell_types_categorical[train_index], cell_types_categorical[test_index]\n",
    "\n",
    "    input_expr = Input(shape=(1,), name='input_expr')\n",
    "    input_emb = Input(shape=(X_train_emb.shape[1],), name='input_emb')\n",
    "    \n",
    "    noisy_expr = GaussianNoise(0.1)(input_expr)\n",
    "    noisy_emb = GaussianNoise(0.1)(input_emb)\n",
    "\n",
    "    pathway_expr = Dense(64, activation='relu')(noisy_expr)\n",
    "    pathway_expr = Dropout(0.3)(pathway_expr)\n",
    "    pathway_expr = Dense(32, activation='relu')(pathway_expr)\n",
    "\n",
    "    pathway_emb = Dense(256, activation='relu')(noisy_emb)\n",
    "    pathway_emb = Dropout(0.3)(pathway_emb)\n",
    "    pathway_emb = Dense(128, activation='relu')(pathway_emb)\n",
    "    \n",
    "    concatenated = Concatenate()([pathway_expr, pathway_emb])\n",
    "\n",
    "    output = Dense(cell_types_categorical.shape[1], activation='softmax')(concatenated)\n",
    "\n",
    "    model = Model(inputs=[input_expr, input_emb], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if fold == 1:\n",
    "        # Only print the model summary for the first fold\n",
    "        model.summary()\n",
    "\n",
    "    print(f\"\\nTraining on fold {fold}...\")\n",
    "    model.fit([X_train_expr, X_train_emb], y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)\n",
    "\n",
    "    loss, accuracy = model.evaluate([X_test_expr, X_test_emb], y_test, verbose=1)\n",
    "    print(f\"Fold {fold} Test Accuracy: {accuracy}\\n\")\n",
    "    scores.append(accuracy)\n",
    "\n",
    "average_score = np.mean(scores)\n",
    "print(f\"\\nAverage Test Accuracy across {n_splits} folds: {average_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
