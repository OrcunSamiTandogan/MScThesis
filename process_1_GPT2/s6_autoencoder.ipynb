{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Step 5: Autoencoder`\n",
    "- Free\n",
    "- Basic\n",
    "- Denoising\n",
    "- Dropout\n",
    "- Combined \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- `Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    if manager == 1:\n",
    "        print(\"s1_load.ipynb running from MANAGER\")\n",
    "except: \n",
    "    %run s0_config.ipynb \n",
    "    subject_autoencoder_0 = \"Autoencoder_v4\" # Embeddings \n",
    "    subject_autoencoder_1 = \"Free_v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- `Load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_name = \"data_raw\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    data_raw = pickle.load(file) \n",
    "\n",
    "import_name = \"subject_data_full\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    subject_data_full = pickle.load(file) \n",
    "\n",
    "\n",
    "import_name = \"subject_outlier_0\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    subject_outlier_0 = pickle.load(file) \n",
    "\n",
    "\n",
    "import_name = \"subject_outlier_1\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    subject_outlier_1 = pickle.load(file) \n",
    "    \n",
    "import_name = \"subject_normalization_0\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    subject_normalization_0 = pickle.load(file) \n",
    "\n",
    "\n",
    "import_name = \"subject_normalization_1\"\n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_normalization}\", f\"{import_name}\" + \".pkl\")) , 'rb') as file:\n",
    "    subject_normalization_1 = pickle.load(file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Subject Outlier 0 (Embedding): {subject_outlier_0}\")\n",
    "print(f\"Subject Outlier 1 (Expression): {subject_outlier_1}\")\n",
    "print(f\"Subject normalization 0 (Embedding): {subject_normalization_0}\")\n",
    "print(f\"Subject normalization 1 (Expression): {subject_normalization_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2- `Preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-) `Overview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Subject Outlier 0 (Embedding): {subject_outlier_0}\")\n",
    "print(f\"Subject Outlier 1 (Expression): {subject_outlier_1}\")\n",
    "print(f\"Subject normalization 0 (Embedding): {subject_normalization_0}\")\n",
    "print(f\"Subject normalization 1 (Expression): {subject_normalization_1}\")\n",
    "print(f\"Subject Autoencoder 0 (Embedding): {subject_autoencoder_0}\")\n",
    "print(f\"Subject Autoencoder 1 (Expression): {subject_autoencoder_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-) `Tool Setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3- `Bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) `Embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-) `Overview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data_full.iloc[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and validation function\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in dataloader:\n",
    "        inputs, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, _ = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_epoch_vae(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in dataloader:\n",
    "        inputs, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(inputs)\n",
    "        loss = vae_loss_function(recon_batch, inputs, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch_vae(model, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, _ = data\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = vae_loss_function(recon_batch, inputs, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Hyperparameter options\n",
    "options_batch_size = [16, 32, 64, 128]\n",
    "options_batch_size = [32]\n",
    "options_encoding_dim = [32, 64, 128, 256]\n",
    "options_encoding_dim = [128]\n",
    "options_loss_function = [\"MSELoss\", \"L1Loss\", \"HuberLoss\"]\n",
    "options_loss_function = [\"HuberLoss\"]\n",
    "options_optimizers = [\"Adam\", \"AdamW\", \"SGD\", \"RMSprop\"]\n",
    "options_optimizers = [\"RMSprop\"]\n",
    "options_learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "options_learning_rates = [0.001]\n",
    "options_scheduler = [\"None\", \"StepLR\", \"ReduceLROnPlateau\", \"ExponentialLR\"]\n",
    "options_scheduler = [\"None\"]\n",
    "options_num_epochs = [10, 100, 200, 500]\n",
    "options_num_epochs = [10] \n",
    "options_model_types = [ \"Free_v0\", \"Autoencoder_v1\", \"Autoencoder_v2\", \"Autoencoder_v3\", \"Autoencoder_v4\", \"Autoencoder_v5\", \n",
    "                       \"BasicAutoencoder_v0\", \"DeepAutoencoder_v0\", \"SparseAutoencoder_v0\", \"ConvAutoencoder_v0\", \"VAE_v0\"]\n",
    "\n",
    "try:\n",
    "    options_model_types = forced_options_model_types\n",
    "except:\n",
    "    options_model_types = [subject_autoencoder_0] #Â [\"Autoencoder_v4\"]\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "total_stat_df = pd.DataFrame()\n",
    "\n",
    "# Normalize the embeddings\n",
    "embeddings = np.array(subject_data_full[\"Embeddings_Normalized\"].tolist())\n",
    "scaler = StandardScaler()\n",
    "embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "embeddings_train, embeddings_val = train_test_split(embeddings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "embeddings_train_tensor = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "embeddings_val_tensor = torch.tensor(embeddings_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset = TensorDataset(embeddings_train_tensor, embeddings_train_tensor)\n",
    "val_dataset = TensorDataset(embeddings_val_tensor, embeddings_val_tensor)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3- `Bench`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_models = access_data_path(f\"{process_barcode}/class/{folder_autoencoder}\", \"models\" + \".ipynb\") \n",
    "%run $autoencoder_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.epochs_no_improve = 0\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_combinations = 0 \n",
    "for model_type in options_model_types:\n",
    "    for batch_size in options_batch_size:\n",
    "        for encoding_dim in options_encoding_dim:\n",
    "            for loss_function in options_loss_function:\n",
    "                for optimizer_name in options_optimizers:\n",
    "                    for learning_rate in options_learning_rates:\n",
    "                        for scheduler_name in options_scheduler:\n",
    "                            for num_epochs in options_num_epochs:\n",
    "                                total_combinations += 1 \n",
    "total_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model dictionary and save models with information. \n",
    "model_dict = {} \n",
    "# Create unique id\n",
    "current_combination = 0\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "early_stopping_min_delta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate combinations of hyperparameters from the provided options\n",
    "for model_type in options_model_types:\n",
    "    for batch_size in options_batch_size:\n",
    "        for encoding_dim in options_encoding_dim:\n",
    "            for loss_function in options_loss_function:\n",
    "                for optimizer_name in options_optimizers:\n",
    "                    for learning_rate in options_learning_rates:\n",
    "                        for scheduler_name in options_scheduler:\n",
    "                            for num_epochs in options_num_epochs:\n",
    "\n",
    "                                print(\"*\" * 100)\n",
    "                                print(f\"Current Combination: {current_combination}\")\n",
    "                                current_combination += 1\n",
    "\n",
    "                                print(f\"Model Type: {model_type}\")\n",
    "                                print(f\"Batch Size: {batch_size}\")\n",
    "                                print(f\"Encoding Dim: {encoding_dim}\")\n",
    "                                print(f\"Loss Function: {loss_function}\")\n",
    "                                print(f\"Optimizer: {optimizer_name}\")\n",
    "                                print(f\"Learning Rate: {learning_rate}\")\n",
    "                                print(f\"Scheduler: {scheduler_name}\")\n",
    "                                print(f\"Num Epochs: {num_epochs}\")\n",
    "\n",
    "                                # Initialize DataLoader\n",
    "                                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                                print(\"*\" * 100)\n",
    "                                print(\"Selected Autoencoder: \", model_type)\n",
    "                                print(\"*\" * 100) \n",
    "                                if model_type == \"Free_v0\":\n",
    "                                    # Directly use normalized embeddings without any autoencoder\n",
    "                                    encoded_data = torch.tensor(embeddings, dtype=torch.float32)\n",
    "                                else:\n",
    "                                    # Initialize the model\n",
    "                                    print(\"Current Model Type:\", model_type)\n",
    "                                    model = get_autoencoder_model(model_type = model_type, input_dim=input_dim, encoding_dim=encoding_dim)\n",
    "\n",
    "                                    # Initialize loss function\n",
    "                                    if loss_function == \"MSELoss\":\n",
    "                                        criterion = nn.MSELoss()\n",
    "                                    elif loss_function == \"L1Loss\":\n",
    "                                        criterion = nn.L1Loss()\n",
    "                                    elif loss_function == \"HuberLoss\":\n",
    "                                        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "                                    # Optimizer initialization\n",
    "                                    if optimizer_name == \"Adam\":\n",
    "                                        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                                    elif optimizer_name == \"AdamW\":\n",
    "                                        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "                                    elif optimizer_name == \"SGD\":\n",
    "                                        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                                    elif optimizer_name == \"RMSprop\":\n",
    "                                        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                                    # Scheduler initialization\n",
    "                                    if scheduler_name == \"StepLR\":\n",
    "                                        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "                                    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "                                        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "                                    elif scheduler_name == \"ExponentialLR\":\n",
    "                                        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "                                    else:\n",
    "                                        scheduler = None\n",
    "\n",
    "                                    # Initialize early stopping\n",
    "                                    early_stopping = EarlyStopping(patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n",
    "\n",
    "                                    # Training loop\n",
    "                                    train_losses = []\n",
    "                                    val_losses = []\n",
    "                                    best_val_loss = float('inf')\n",
    "                                    best_model_state = None\n",
    "\n",
    "                                    try:\n",
    "                                        for epoch in range(num_epochs):\n",
    "                                            train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "                                            val_loss = validate_epoch(model, val_loader, criterion)\n",
    "                                            train_losses.append(train_loss)\n",
    "                                            val_losses.append(val_loss)\n",
    "                                            if scheduler is not None:\n",
    "                                                if scheduler_name == \"ReduceLROnPlateau\":\n",
    "                                                    scheduler.step(val_loss)\n",
    "                                                else:\n",
    "                                                    scheduler.step()\n",
    "                                            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "                                            \n",
    "                                            # Save the best model\n",
    "                                            if val_loss < best_val_loss:\n",
    "                                                best_val_loss = val_loss\n",
    "                                                best_model_state = model.state_dict()\n",
    "\n",
    "                                            # Check early stopping condition\n",
    "                                            early_stopping(val_loss)\n",
    "                                            if early_stopping.early_stop:\n",
    "                                                print(\"Early stopping triggered\")\n",
    "                                                break\n",
    "\n",
    "                                        # Load the best model state before saving it to the model dictionary\n",
    "                                        if best_model_state is not None:\n",
    "                                            model.load_state_dict(best_model_state)\n",
    "\n",
    "                                        # Collecting statistics\n",
    "                                        model_id = \"Selected_Autoencoder\"\n",
    "                                        stat_dict = {\n",
    "                                            \"model_id\": model_id,\n",
    "                                            \"model_type\": model_type,\n",
    "                                            \"train_first\": train_losses[0],\n",
    "                                            \"train_last\": train_losses[-1],\n",
    "                                            \"val_first\": val_losses[0],\n",
    "                                            \"val_last\": val_losses[-1],\n",
    "                                            \"batch_size\": batch_size,\n",
    "                                            \"encoding_dim\": encoding_dim,\n",
    "                                            \"loss_function\": loss_function,\n",
    "                                            \"optimizer\": optimizer_name,\n",
    "                                            \"learning_rate\": learning_rate,\n",
    "                                            \"scheduler\": scheduler_name,\n",
    "                                            \"num_epochs\": num_epochs\n",
    "                                        }\n",
    "                                        model_dict[model_id] = model\n",
    "\n",
    "                                        # Append results to the DataFrame\n",
    "                                        df = pd.DataFrame(stat_dict, index=[0])\n",
    "                                        total_stat_df = pd.concat([total_stat_df, df], axis=0)\n",
    "                                        \n",
    "                                        # Save the model\n",
    "                                        torch.save(model.state_dict(), f\"./model/autoencoder/selected.pth\") \n",
    "                                        \n",
    "                                        config_file = {\"input_dim\":input_dim, \"encoding_dim\":encoding_dim, \"model_type\":model_type}\n",
    "                                        export_name = \"config_file\" \n",
    "                                        with open(os.path.join(access_data_path(f\"{process_barcode}/model/autoencoder/\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "                                            pickle.dump(config_file, file)   \n",
    "                                        \n",
    "                                        \n",
    "                                        # Encode the data and save it\n",
    "                                        full_dataset = TensorDataset(torch.tensor(embeddings, dtype=torch.float32))\n",
    "                                        encoded_data = []\n",
    "                                        model.eval()\n",
    "                                        with torch.no_grad():\n",
    "                                            for data in DataLoader(full_dataset, batch_size=1):\n",
    "                                                inputs = data[0]\n",
    "                                                encoded_output = model.encoder(inputs)\n",
    "                                                encoded_data.append(encoded_output)\n",
    "                                        \n",
    "                                        # Convert encoded data to a single tensor\n",
    "                                        encoded_data = torch.cat(encoded_data)\n",
    "\n",
    "                                        print(f\"Encoded data shape: {encoded_data.shape}\")  # Debug statement\n",
    "\n",
    "                                        # Ensure the dimensions match\n",
    "                                        assert encoded_data.shape[0] == len(subject_data_full), f\"Row count mismatch: encoded_data has {encoded_data.shape[0]} rows, subject_data_full has {len(subject_data_full)} rows\"\n",
    "\n",
    "                                        # Save encoded data\n",
    "                                        #torch.save(encoded_data, f\"{model_id}_encoded_data.pth\")\n",
    "\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"An error occurred: {e}\")\n",
    "                                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Shape {subject_data_full.shape}\")\n",
    "print(f\"Current Shape {subject_data_full['Embeddings_Normalized'].iloc[0].shape}\")\n",
    "\n",
    "print(f\"Tensor Shape {encoded_data.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a list of lists\n",
    "encoded_data_list = encoded_data.tolist()\n",
    "\n",
    "# Ensure the dimensions match\n",
    "assert len(encoded_data_list) == len(subject_data_full), \"Row count mismatch between encoded_data and DataFrame\"\n",
    "\n",
    "# Add as a new column\n",
    "subject_data_full['Embeddings_Autoencoder'] = encoded_data_list\n",
    "\n",
    "subject_data_full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Visuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = subject_data_full[\"Embeddings_Normalized\"].tolist()\n",
    "raw_data_np = np.array(embeddings_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming encoded_data is derived from encoded_data\n",
    "encoded_data_np = encoded_data.numpy()\n",
    "# Extract labels from subject_data_full\n",
    "cell_type_labels = subject_data_full[\"Cell_Type\"].values[:len(encoded_data_np)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2D - UMAP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_umap = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "    # Apply 2D UMAP\n",
    "    umap_2d_raw = UMAP(n_components=2, random_state=42)\n",
    "    raw_umap_2d = umap_2d_raw.fit_transform(raw_data_np)\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels = subject_data_full[\"Cell_Type\"].values\n",
    "    # Verify the shape of the UMAP output\n",
    "    print(f\"Shape of raw_umap_2d: {raw_umap_2d.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Plot UMAP visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=raw_umap_2d[:, 0], y=raw_umap_2d[:, 1], hue=cell_type_labels, palette='tab20', s=50, alpha=0.7)\n",
    "    plt.title('UMAP Visualization of Raw Gene Marker Data')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.legend(title='Cell Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Verify lengths\n",
    "    print(f\"Length of encoded_data_np: {encoded_data_np.shape[0]}\")\n",
    "    print(f\"Length of cell_type_labels: {len(cell_type_labels)}\") \n",
    "    # Apply 2D UMAP\n",
    "    umap_2d_encoded = UMAP(n_components=2, random_state=42)\n",
    "    encoded_umap_2d = umap_2d_encoded.fit_transform(encoded_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Plot UMAP visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=encoded_umap_2d[:, 0], y=encoded_umap_2d[:, 1], hue=cell_type_labels, palette='tab20', s=50, alpha=0.7)\n",
    "    plt.title('UMAP Visualization of Encoded Gene Marker Data')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.legend(title='Cell Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3D - UMAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Apply 3D UMAP\n",
    "    umap_3d_raw = UMAP(n_components=3, random_state=42)\n",
    "    raw_umap_3d = umap_3d_raw.fit_transform(raw_data_np)\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels_raw = subject_data_full[\"Cell_Type\"].values[:len(raw_umap_3d)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Create a DataFrame for Plotly\n",
    "    df_raw = pd.DataFrame({\n",
    "        'x': raw_umap_3d[:, 0],\n",
    "        'y': raw_umap_3d[:, 1],\n",
    "        'z': raw_umap_3d[:, 2],\n",
    "        'label': cell_type_labels_raw\n",
    "    })\n",
    "    # Plot 3D UMAP visualization\n",
    "    fig_raw = px.scatter_3d(df_raw, x='x', y='y', z='z', \n",
    "                            color='label', \n",
    "                            title='3D UMAP Visualization of Raw Gene Marker Data')\n",
    "\n",
    "    # Update the marker size\n",
    "    fig_raw.update_traces(marker=dict(size=3))  # Set the size to a smaller value\n",
    "\n",
    "    # Show plot\n",
    "    fig_raw.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Assuming encoded_data is derived from encoded_data\n",
    "    encoded_data_np = encoded_data.numpy()\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels = subject_data_full[\"Cell_Type\"].values[:len(encoded_data_np)]\n",
    "    # Apply 3D UMAP\n",
    "    umap_3d_encoded = UMAP(n_components=3, random_state=42)\n",
    "    encoded_umap_3d = umap_3d_encoded.fit_transform(encoded_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Create a DataFrame for Plotly\n",
    "    df_encoded = pd.DataFrame({\n",
    "        'x': encoded_umap_3d[:, 0],\n",
    "        'y': encoded_umap_3d[:, 1],\n",
    "        'z': encoded_umap_3d[:, 2],\n",
    "        'label': cell_type_labels\n",
    "    })\n",
    "    # Plot 3D UMAP visualization\n",
    "    fig_encoded = px.scatter_3d(df_encoded, x='x', y='y', z='z', \n",
    "                                color='label', \n",
    "                                title='3D UMAP Visualization of Encoded Gene Marker Data')\n",
    "\n",
    "    # Update the marker size\n",
    "    fig_encoded.update_traces(marker=dict(size=3))  # Set the size to a smaller value\n",
    "\n",
    "    # Show plot\n",
    "    fig_encoded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2D - TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Convert the embeddings column to a 2D NumPy array\n",
    "    embeddings_list = subject_data_full[\"Embeddings\"].tolist()\n",
    "    raw_data_np = np.array(embeddings_list)\n",
    "\n",
    "    # Verify the shape of the array\n",
    "    print(f\"Shape of raw_data_np: {raw_data_np.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Apply 2D t-SNE\n",
    "    tsne_2d_raw = TSNE(n_components=2, random_state=42)\n",
    "    raw_tsne_2d = tsne_2d_raw.fit_transform(raw_data_np)\n",
    "\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels = subject_data_full[\"Cell_Type\"].values\n",
    "\n",
    "    # Verify the shape of the t-SNE output\n",
    "    print(f\"Shape of raw_tsne_2d: {raw_tsne_2d.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Plot t-SNE visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=raw_tsne_2d[:, 0], y=raw_tsne_2d[:, 1], hue=cell_type_labels, palette='tab20', s=50, alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Raw Gene Marker Data')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(title='Cell Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Assuming encoded_data_np is derived from encoded_data\n",
    "    encoded_data_np = encoded_data.numpy()\n",
    "\n",
    "    # Extract labels from subject_data_full\n",
    "    cell_type_labels = subject_data_full[\"Cell_Type\"].values[:len(encoded_data_np)]\n",
    "\n",
    "    # Verify lengths\n",
    "    print(f\"Length of encoded_data_np: {encoded_data_np.shape[0]}\")\n",
    "    print(f\"Length of cell_type_labels: {len(cell_type_labels)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Apply 2D t-SNE\n",
    "    tsne_2d_encoded = TSNE(n_components=2, random_state=42)\n",
    "    encoded_tsne_2d = tsne_2d_encoded.fit_transform(encoded_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Plot t-SNE visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=encoded_tsne_2d[:, 0], y=encoded_tsne_2d[:, 1], hue=cell_type_labels, palette='tab20', s=50, alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Encoded Gene Marker Data')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(title='Cell Types', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3D - TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Assuming encoded_data_np is derived from encoded_data\n",
    "    encoded_data_np = encoded_data.numpy()\n",
    "\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels = subject_data_full[\"Cell_Type\"].values[:len(encoded_data_np)]\n",
    "\n",
    "    # Create a mapping from cell type labels to numeric values\n",
    "    unique_labels = np.unique(cell_type_labels)\n",
    "    label_to_num = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    numeric_labels = np.array([label_to_num[label] for label in cell_type_labels])\n",
    "\n",
    "    # Verify the mapping\n",
    "    print(f\"Label to Numeric Mapping: {label_to_num}\")\n",
    "    print(f\"Numeric Labels: {numeric_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Apply 3D t-SNE\n",
    "    tsne_3d_raw = TSNE(n_components=3, random_state=42)\n",
    "    raw_tsne_3d = tsne_3d_raw.fit_transform(raw_data_np)\n",
    "\n",
    "    # Ensure the cell type labels are the correct length\n",
    "    cell_type_labels_raw = subject_data_full[\"Cell_Type\"].values[:len(raw_tsne_3d)]\n",
    "\n",
    "    # Verify lengths to ensure they match\n",
    "    print(f\"Length of raw_tsne_3d: {raw_tsne_3d.shape[0]}\")\n",
    "    print(f\"Length of cell_type_labels_raw: {len(cell_type_labels_raw)}\")\n",
    "\n",
    "    # Create a DataFrame for Plotly\n",
    "    df_raw = pd.DataFrame({\n",
    "        'x': raw_tsne_3d[:, 0],\n",
    "        'y': raw_tsne_3d[:, 1],\n",
    "        'z': raw_tsne_3d[:, 2],\n",
    "        'label': cell_type_labels_raw\n",
    "    })\n",
    "\n",
    "    # Plot 3D t-SNE visualization\n",
    "    fig_raw = px.scatter_3d(df_raw, x='x', y='y', z='z', \n",
    "                            color='label', \n",
    "                            title='3D t-SNE Visualization of Raw Gene Marker Data')\n",
    "\n",
    "    # Update the marker size\n",
    "    fig_raw.update_traces(marker=dict(size=3))  # Set the size to a smaller value\n",
    "\n",
    "    # Show plot\n",
    "    fig_raw.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if switch_umap == 1: \n",
    "\n",
    "    # Apply 3D t-SNE\n",
    "    tsne_3d_encoded = TSNE(n_components=3, random_state=42)\n",
    "    encoded_tsne_3d = tsne_3d_encoded.fit_transform(encoded_data_np)\n",
    "\n",
    "    # Verify lengths to ensure they match\n",
    "    print(f\"Length of encoded_tsne_3d: {encoded_tsne_3d.shape[0]}\")\n",
    "    print(f\"Length of cell_type_labels: {len(cell_type_labels)}\")\n",
    "\n",
    "    # Create a DataFrame for Plotly\n",
    "    df_encoded = pd.DataFrame({\n",
    "        'x': encoded_tsne_3d[:, 0],\n",
    "        'y': encoded_tsne_3d[:, 1],\n",
    "        'z': encoded_tsne_3d[:, 2],\n",
    "        'label': cell_type_labels\n",
    "    })\n",
    "\n",
    "    # Plot 3D t-SNE visualization\n",
    "    fig_encoded = px.scatter_3d(df_encoded, x='x', y='y', z='z', \n",
    "                                color='label', \n",
    "                                title='3D t-SNE Visualization of Encoded Gene Marker Data')\n",
    "\n",
    "    # Update the marker size\n",
    "    fig_encoded.update_traces(marker=dict(size=3))  # Set the size to a smaller value\n",
    "\n",
    "    # Show plot\n",
    "    fig_encoded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Rest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    subject_data_full[\"Expression_Autoencoder\"] = subject_data_full[\"Expression\"] \n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4- `End`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name = \"data_raw\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(data_raw, file)   \n",
    "\n",
    "export_name = \"subject_data_full\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_data_full, file)   \n",
    "\n",
    "    \n",
    "export_name = \"subject_outlier_0\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_outlier_0, file)   \n",
    "\n",
    "export_name = \"subject_outlier_1\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_outlier_1, file)   \n",
    "\n",
    "\n",
    "\n",
    "export_name = \"subject_normalization_0\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_normalization_0, file)   \n",
    "\n",
    "export_name = \"subject_normalization_1\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_normalization_1, file)   \n",
    "\n",
    "\n",
    "\n",
    "export_name = \"subject_autoencoder_0\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_autoencoder_0, file)   \n",
    "\n",
    "export_name = \"subject_autoencoder_1\" \n",
    "with open(os.path.join(access_data_path(f\"{process_barcode}/data/{folder_autoencoder}\", f\"{export_name}\" + \".pkl\"))  , 'wb') as file: \n",
    "    pickle.dump(subject_autoencoder_1, file)   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
